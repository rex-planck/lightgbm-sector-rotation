import qlib
from qlib.constant import REG_CN
from qlib.utils import init_instance_by_config
from qlib.workflow import R
import pandas as pd
import numpy as np
import gc
import os
from gplearn.genetic import SymbolicRegressor

# 1. å±è”½è­¦å‘Š & çŽ¯å¢ƒè®¾ç½®
import warnings

warnings.filterwarnings("ignore")
os.environ["PYTHONWARNINGS"] = "ignore"

# 2. åˆå§‹åŒ– Qlib
print("ðŸ› ï¸ åˆå§‹åŒ– Qlib æ•°æ®å¼•æ“Ž...")
provider_uri = r"E:\Quant_program\Qlib-Cache\cn_data"
qlib.init(provider_uri=provider_uri, region=REG_CN)

# 3. å‡†å¤‡æ•°æ®é…ç½®
market = "csi300"
benchmark = "SH000300"

# æˆ‘ä»¬ä½¿ç”¨ 2018-2020 ä½œä¸ºè®­ç»ƒé›†ï¼ˆæŒ–æŽ˜ï¼‰ï¼Œ2021 ä½œä¸ºéªŒè¯é›†
TRAIN_START = "2018-01-01"
TRAIN_END = "2020-12-31"
TEST_START = "2021-01-01"
TEST_END = "2021-12-31"

data_handler_config = {
    "start_time": TRAIN_START,
    "end_time": TEST_END,
    "fit_start_time": TRAIN_START,
    "fit_end_time": TRAIN_END,
    "instruments": market,
    # æŒ–æŽ˜æ—¶ä¸éœ€è¦å¤æ‚çš„é¢„å¤„ç†ï¼Œåªéœ€æ ‡å‡†åŒ–å’Œå¡«å……
    "infer_processors": [
        {"class": "RobustZScoreNorm", "kwargs": {"fields_group": "feature", "clip_outlier": True}},
        {"class": "Fillna", "kwargs": {"fields_group": "feature"}},
    ],
    "learn_processors": [
        {"class": "DropnaLabel"},
        {"class": "CSRankNorm", "kwargs": {"fields_group": "label"}},
    ],
    # é¢„æµ‹ç›®æ ‡ï¼šæœªæ¥ 5 æ—¥æ”¶ç›ŠçŽ‡
    "label": ["Ref($close, -5) / $close - 1"],
}

dataset_config = {
    "class": "DatasetH",
    "module_path": "qlib.data.dataset",
    "kwargs": {
        "handler": {
            "class": "Alpha158",
            "module_path": "qlib.contrib.data.handler",
            "kwargs": data_handler_config,
        },
        "segments": {
            "train": (TRAIN_START, TRAIN_END),
            "test": (TEST_START, TEST_END),
        },
    },
}


def clean_data(df):
    """æš´åŠ›æ¸…æ´—æ•°æ®ï¼šåŽ»é™¤ Inf, NaN"""
    # æ›¿æ¢æ— ç©·å¤§ä¸º NaN
    df = df.replace([np.inf, -np.inf], np.nan)
    # å¡«å…… NaN ä¸º 0 (ç®€å•ç²—æš´ï¼Œä½†å¯¹ gplearn å¾ˆæœ‰æ•ˆ)
    df = df.fillna(0)
    return df


def run_mining():
    print("â³ æ­£åœ¨åŠ è½½ Alpha158 æ•°æ® (è¿™å¯èƒ½éœ€è¦ 1-2 åˆ†é’Ÿ)...")
    dataset = init_instance_by_config(dataset_config)

    # èŽ·å– DataFrame
    # è¿™é‡Œçš„ 'learn' åŒ…å«äº† feature å’Œ label
    df_train = dataset.prepare("train", col_set=["feature", "label"], data_key="learn")
    df_test = dataset.prepare("test", col_set=["feature", "label"], data_key="learn")

    print(f"   åŽŸå§‹è®­ç»ƒé›†è§„æ¨¡: {df_train.shape}")

    # --- å…³é”®æ­¥éª¤ï¼šæ•°æ®æ¸…æ´— ---
    print("ðŸ§¹ æ­£åœ¨æ¸…æ´—æ•°æ® (Removing NaNs/Infs)...")
    df_train = clean_data(df_train)
    df_test = clean_data(df_test)

    # åˆ†ç¦»ç‰¹å¾ (X) å’Œ æ ‡ç­¾ (y)
    # feature åˆ—åé€šå¸¸æ˜¯ KMID, KLOW ç­‰ï¼Œlabel æ˜¯æœ€åŽä¸€åˆ—
    # æˆ‘ä»¬é€šè¿‡ level 0 æ¥åŒºåˆ†ï¼Œæˆ–è€…ç›´æŽ¥åˆ‡ç‰‡
    # Alpha158 çš„ç‰¹å¾åˆ—åæ¯”è¾ƒé•¿ï¼Œè¿™é‡Œæˆ‘ä»¬å‡è®¾å‰ 158 åˆ—æ˜¯ç‰¹å¾
    X_train = df_train.iloc[:, :-1]
    y_train = df_train.iloc[:, -1]

    X_test = df_test.iloc[:, :-1]
    y_test = df_test.iloc[:, -1]

    # --- å…³é”®æ­¥éª¤ï¼šé™é‡‡æ · (Downsampling) ---
    # å¦‚æžœæ•°æ®é‡è¶…è¿‡ 10ä¸‡è¡Œï¼Œæˆ‘ä»¬éšæœºæŠ½å– 5ä¸‡è¡Œæ¥è®­ç»ƒ
    # è¿™æ ·èƒ½æžå¤§åŠ é€Ÿé—ä¼ ç®—æ³•ï¼ŒåŒæ—¶ä¸æŸå¤±å¤ªå¤šç²¾åº¦
    # ä½ çš„ç”µè„‘æœ‰ 32G å†…å­˜ï¼Œæˆ‘ä»¬å¯ä»¥ç¨å¾®å¤§èƒ†ç‚¹ï¼Œç”¨ 10ä¸‡è¡Œ
    SAMPLE_SIZE = 100000

    if len(X_train) > SAMPLE_SIZE:
        print(f"âœ‚ï¸ æ•°æ®é‡è¿‡å¤§ï¼Œè¿›è¡Œéšæœºé™é‡‡æ ·è‡³ {SAMPLE_SIZE} è¡Œ (ä¸ºäº†åŠ é€Ÿè¿›åŒ–)...")
        # ä¿æŒéšæœºç§å­ä¸€è‡´
        sample_idx = np.random.choice(len(X_train), SAMPLE_SIZE, replace=False)
        X_train_sample = X_train.iloc[sample_idx]
        y_train_sample = y_train.iloc[sample_idx]
    else:
        X_train_sample = X_train
        y_train_sample = y_train

    # ä¿å­˜åˆ—åï¼Œæ–¹ä¾¿åŽç»­æŸ¥é˜…å…¬å¼é‡Œçš„ X0, X1 æ˜¯è°
    feature_names = X_train.columns.tolist()

    # é‡Šæ”¾å†…å­˜
    del df_train
    gc.collect()

    # é…ç½®é—ä¼ è§„åˆ’
    print("\nðŸ§¬ é…ç½®é—ä¼ è¿›åŒ–å¼•æ“Ž (Symbolic Regressor)...")
    # æˆ‘ä»¬å®šä¹‰ä¸€äº›é€‚åˆé‡‘èžçš„å‡½æ•°é›†
    function_set = ['add', 'sub', 'mul', 'div', 'sqrt', 'log', 'abs', 'neg', 'max', 'min']

    print("\nðŸ§¬ é…ç½®é—ä¼ è¿›åŒ–å¼•æ“Ž (Symbolic Regressor)...")
    # æˆ‘ä»¬å®šä¹‰ä¸€äº›é€‚åˆé‡‘èžçš„å‡½æ•°é›†
    function_set = ['add', 'sub', 'mul', 'div', 'sqrt', 'log', 'abs', 'neg', 'max', 'min']

    est_gp = SymbolicRegressor(
        population_size=2000,
        generations=20,  # ðŸ”¥ å¢žåŠ åˆ° 20 ä»£ï¼Œç»™å®ƒæ›´å¤šæ—¶é—´è¿›åŒ–
        tournament_size=20,
        stopping_criteria=1.0,  # ç›¸å…³ç³»æ•°æœ€å¤§æ˜¯1ï¼Œè®¾ä¸ªè¾¾ä¸åˆ°çš„å€¼è®©å®ƒä¸€ç›´è·‘
        p_crossover=0.4,  # é™ä½Žæ‚äº¤ï¼Œå¢žåŠ çªå˜
        p_subtree_mutation=0.1,
        p_hoist_mutation=0.05,
        p_point_mutation=0.1,
        max_samples=0.9,
        verbose=1,
        parsimony_coefficient=0.0001,  # ðŸ”¥ é™ä½Žæƒ©ç½šï¼Œå…è®¸å…¬å¼å˜å¤æ‚ä¸€ç‚¹ï¼Œåˆ«è€æ˜¯ç”¨ 0
        random_state=42,
        function_set=function_set,
        metric='spearman',  # ðŸ”¥ðŸ”¥ðŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šç›´æŽ¥ä¼˜åŒ– Rank ICï¼Œè€Œä¸æ˜¯è¯¯å·®ï¼
        n_jobs=1
    )

    print("ðŸš€ å¼€å§‹æŒ–æŽ˜ (Mining Started)... è¯·è€å¿ƒç­‰å¾…æ¯ä¸€ä»£çš„è¿›åº¦è¾“å‡º")
    est_gp.fit(X_train_sample, y_train_sample)

    # --- ç»“æžœåˆ†æž ---
    print("\n" + "=" * 50)
    print("ðŸ† æŒ–æŽ˜ç»“æžœ (Top Factor):")
    print("=" * 50)
    print(f"æœ€å¼ºå…¬å¼ (Raw): {est_gp._program}")

    # å°è¯•åœ¨æµ‹è¯•é›†ä¸ŠéªŒè¯æ•ˆæžœ
    print("\nðŸ“ˆ æ­£åœ¨æµ‹è¯•é›†ä¸Šå›žæµ‹å› å­è¡¨çŽ° (2021å¹´)...")
    # æ³¨æ„ï¼šé¢„æµ‹æ—¶è¦ç”¨å…¨é‡æµ‹è¯•é›†ï¼Œä¸è¦é‡‡æ ·
    y_pred = est_gp.predict(X_test)

    # è®¡ç®— Rank IC
    res_df = pd.DataFrame({'pred': y_pred, 'label': y_test.values})
    rank_ic = res_df.rank().corr().iloc[0, 1]

    print("-" * 50)
    print(f"ðŸ“Š å› å­æ ·æœ¬å¤–æµ‹è¯• (Out-of-Sample Test):")
    print(f"   Rank IC: {rank_ic:.4%}")
    print("-" * 50)

    if rank_ic > 0.03:
        print("ðŸŽ‰ æ­å–œï¼ä½ æŒ–åˆ°äº†ä¸€ä¸ªæœ‰æ•ˆçš„ Alpha å› å­ï¼")
        print("   (åœ¨çº¯æœºå™¨æŒ–æŽ˜ä¸­ï¼ŒOOS IC > 3% å·²ç»éžå¸¸ä¸é”™äº†)")
    else:
        print("ðŸ’ª æ•ˆæžœä¸€èˆ¬ï¼Œå¯èƒ½å‡ºçŽ°äº†è¿‡æ‹Ÿåˆã€‚å¯ä»¥å°è¯•è°ƒå¤§ parsimony_coefficient æˆ–å¢žåŠ  generationsã€‚")

    # å°è¯•è§£æžå…¬å¼ä¸­çš„ X
    print("\nðŸ” å…¬å¼ç‰¹å¾è§£æžæç¤º:")
    print("   gplearn è¾“å‡ºçš„ X0, X1... å¯¹åº”ä»¥ä¸‹ Alpha158 ç‰¹å¾:")
    for i in range(min(10, len(feature_names))):
        print(f"   X{i} -> {feature_names[i]}")
    print("   ... (æ›´å¤šè¯·æŸ¥é˜… qlib Alpha158 æ–‡æ¡£)")


if __name__ == "__main__":
    run_mining()