diff --git a/python_program/05_model_gru.py b/python_program/05_model_gru.py
index a28ea27..d3f6015 100644
--- a/python_program/05_model_gru.py
+++ b/python_program/05_model_gru.py
@@ -2,147 +2,214 @@ import qlib
 from qlib.constant import REG_CN
 from qlib.utils import init_instance_by_config
 from qlib.workflow import R
-# ç›´æ¥ä½¿ç”¨å®˜æ–¹ç±»ï¼Œä¸æè‡ªå®šä¹‰ç»§æ‰¿äº†
-from qlib.data.dataset.handler import DataHandlerLP
+import torch
+import torch.nn as nn
+import torch.optim as optim
+from torch.utils.data import Dataset, DataLoader
 import pandas as pd
+import numpy as np
 import os
 
 # 1. åˆå§‹åŒ–
 provider_uri = r"E:\Quant_program\Qlib-Cache\cn_data"
 qlib.init(provider_uri=provider_uri, region=REG_CN)
 
-# 2. å®šä¹‰è‡ªå®šä¹‰å› å­ (çº¯å…¬å¼åˆ—è¡¨)
-custom_factors = [
-    "( $close - Mean($close, 5) ) / Mean($close, 5)",  # 5æ—¥ä¹–ç¦»
-    "$volume / Mean($volume, 20)",  # é‡æ¯”
-    "Std($close, 20) / $close",  # æ³¢åŠ¨ç‡
-    "($open - Ref($close, 1)) / Ref($close, 1)",  # éš”å¤œè·³ç©º
-    "($close - $open) / $open"  # æ—¥å†…åŠ›åº¦
-]
 
-# 3. é…ç½®å·¥ä½œæµ
+# 2. PyTorch GRU æ¨¡å‹ (å¢åŠ ä¸€ç‚¹ç¨³å®šæ€§)
+class SimpleGRU(nn.Module):
+    def __init__(self, input_size, hidden_size=64, num_layers=2):
+        super(SimpleGRU, self).__init__()
+        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)  # åŠ ç‚¹ Dropout
+        self.fc = nn.Linear(hidden_size, 1)
+
+    def forward(self, x):
+        out, _ = self.gru(x)
+        pred = self.fc(out[:, -1, :])
+        return pred.squeeze()
+
+
+# 3. æ»‘çª— Dataset (ä¿æŒä¸å˜ï¼Œé€»è¾‘æ²¡é—®é¢˜)
+class RollingDataset(Dataset):
+    def __init__(self, df, step_len=20):
+        self.step_len = step_len
+        self.data_values = df.values
+        self.index_map = []
+
+        # æŒ‰ instrument åˆ†ç»„è®¡ç®—åˆ‡ç‰‡ç´¢å¼•
+        # è¿™é‡Œä¸ºäº†é˜²æ­¢ df index ä¸è§„èŒƒï¼Œæˆ‘ä»¬å°è¯• reset_index å† groupby
+        # ä½† Qlib æ•°æ®é€šå¸¸æ˜¯ MultiIndexï¼Œç›´æ¥ groupby(level='instrument') å³å¯
+        try:
+            grouped = df.groupby(level='instrument')
+        except TypeError:
+            # å¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœç´¢å¼•æœ‰é—®é¢˜ï¼Œå¼ºåˆ¶é‡ç½®
+            df_temp = df.reset_index()
+            grouped = df_temp.groupby('instrument')
+
+        current_idx = 0
+        for name, group in grouped:
+            group_len = len(group)
+            if group_len > step_len:
+                valid_starts = np.arange(current_idx, current_idx + group_len - step_len + 1)
+                self.index_map.append(valid_starts)
+            current_idx += group_len
+
+        if len(self.index_map) > 0:
+            self.index_map = np.concatenate(self.index_map)
+        else:
+            self.index_map = np.array([])
+
+    def __len__(self):
+        return len(self.index_map)
+
+    def __getitem__(self, idx):
+        start_row = self.index_map[idx]
+        end_row = start_row + self.step_len
+        window = self.data_values[start_row:end_row]
+
+        feature = window[:, :-1]
+        label = window[-1, -1]
+
+        return torch.tensor(feature, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)
+
+
+# 4. é…ç½®
 market = "csi300"
-benchmark = "SH000300"
-FIT_START = "2018-01-01"
-FIT_END = "2020-12-31"
-
 data_handler_config = {
-    "start_time": "2018-01-01",
+    "start_time": "2015-01-01",
     "end_time": "2022-12-31",
-
-    # âš ï¸ã€å…³é”®ä¿®æ”¹ã€‘è¿™é‡Œåƒä¸‡ä¸è¦å†™ fit_start_timeï¼
-    # å¦‚æœå†™äº†ï¼ŒDataHandlerLP å°±ä¼šæŠ¥é”™ "unexpected argument"ã€‚
-    # "fit_start_time": FIT_START,  <-- åˆ é™¤
-    # "fit_end_time": FIT_END,      <-- åˆ é™¤
-
+    "fit_start_time": "2015-01-01",
+    "fit_end_time": "2020-12-31",
     "instruments": market,
-
-    # æ•°æ®åŠ è½½é…ç½®
-    "data_loader": {
-        "class": "QlibDataLoader",
-        "module_path": "qlib.data.dataset.loader",
-        "kwargs": {
-            "config": {
-                "feature": custom_factors,
-                "label": ["Ref($close, -5) / $close - 1"],
-            },
-        },
-    },
-
-    # ğŸ”¥ğŸ”¥ğŸ”¥ã€å…³é”®ä¿®æ”¹ã€‘åœ¨è¿™é‡Œæ˜¾å¼æ³¨å…¥æ—¶é—´å‚æ•° ğŸ”¥ğŸ”¥ğŸ”¥
-    # è¿™æ ·åº•å±‚å¤„ç†å™¨èƒ½æ‹¿åˆ°å‚æ•°ï¼Œè€Œä¸Šå±‚ Handler åˆä¸ä¼šæŠ¥é”™
     "infer_processors": [
-        {
-            "class": "RobustZScoreNorm",
-            "kwargs": {
-                "fields_group": "feature",
-                "clip_outlier": True,
-                "fit_start_time": FIT_START,  # ğŸ‘ˆ è¿™é‡Œå¿…é¡»ç»™
-                "fit_end_time": FIT_END  # ğŸ‘ˆ è¿™é‡Œå¿…é¡»ç»™
-            }
-        },
+        {"class": "RobustZScoreNorm", "kwargs": {"fields_group": "feature", "clip_outlier": True}},
         {"class": "Fillna", "kwargs": {"fields_group": "feature"}},
     ],
     "learn_processors": [
         {"class": "DropnaLabel"},
-        {
-            "class": "CSRankNorm",
-            "kwargs": {
-                "fields_group": "label",
-                "fit_start_time": FIT_START,  # ğŸ‘ˆ è¿™é‡Œå¿…é¡»ç»™
-                "fit_end_time": FIT_END  # ğŸ‘ˆ è¿™é‡Œå¿…é¡»ç»™
-            }
-        },
+        {"class": "CSRankNorm", "kwargs": {"fields_group": "label"}},
     ],
+    "label": ["Ref($close, -5) / $close - 1"],
 }
 
-task = {
-    "model": {
-        "class": "LGBModel",
-        "module_path": "qlib.contrib.model.gbdt",
-        "kwargs": {
-            "loss": "mse",
-            "colsample_bytree": 0.8879,
-            "learning_rate": 0.0421,
-            "subsample": 0.8789,
-            "lambda_l1": 205.6999,
-            "lambda_l2": 580.9768,
-            "max_depth": 8,
-            "num_leaves": 210,
-            "num_threads": 20,
+dataset_config = {
+    "class": "DatasetH",
+    "module_path": "qlib.data.dataset",
+    "kwargs": {
+        "handler": {
+            "class": "Alpha158",
+            "module_path": "qlib.contrib.data.handler",
+            "kwargs": data_handler_config,
         },
-    },
-    "dataset": {
-        "class": "DatasetH",
-        "module_path": "qlib.data.dataset",
-        "kwargs": {
-            "handler": {
-                # ç›´æ¥ç”¨å®˜æ–¹ç±»ï¼Œé€šè¿‡é…ç½®è§£å†³é—®é¢˜
-                "class": "DataHandlerLP",
-                "module_path": "qlib.data.dataset.handler",
-                "kwargs": data_handler_config,
-            },
-            "segments": {
-                "train": (FIT_START, FIT_END),
-                "valid": ("2021-01-01", "2021-12-31"),
-                "test": ("2022-01-01", "2022-12-31"),
-            },
+        "segments": {
+            "train": ("2015-01-01", "2020-12-31"),
+            "valid": ("2021-01-01", "2021-12-31"),
+            "test": ("2022-01-01", "2022-12-31"),
         },
     },
 }
 
+
+def run_training():
+    with R.start(experiment_name="gru_rolling_window_stable"):
+        print("ğŸ› ï¸ åˆå§‹åŒ–æ•°æ®...")
+        qlib_dataset = init_instance_by_config(dataset_config)
+        df_train = qlib_dataset.prepare("train", col_set=["feature", "label"], data_key="infer")
+        df_valid = qlib_dataset.prepare("valid", col_set=["feature", "label"], data_key="infer")
+        df_test = qlib_dataset.prepare("test", col_set=["feature", "label"], data_key="infer")
+
+        print(f"   Train shape: {df_train.shape}")
+
+        print("ğŸ”„ æ„å»ºæ•°æ®é›†...")
+        train_set = RollingDataset(df_train, step_len=20)
+        valid_set = RollingDataset(df_valid, step_len=20)
+        test_set = RollingDataset(df_test, step_len=20)
+
+        # å¢å¤§ Batch Size ä¹Ÿèƒ½ç¨å¾®ç¨³ä¸€ç‚¹
+        train_loader = DataLoader(train_set, batch_size=1024, shuffle=True, num_workers=0)
+        valid_loader = DataLoader(valid_set, batch_size=1024, shuffle=False, num_workers=0)
+        test_loader = DataLoader(test_set, batch_size=1024, shuffle=False, num_workers=0)
+
+        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+        print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
+
+        model = SimpleGRU(input_size=158, hidden_size=64).to(device)
+        criterion = nn.MSELoss()
+        # ğŸ”¥ é™ä½å­¦ä¹ ç‡ï¼Œé˜²æ­¢æ­¥å­è¿ˆå¤ªå¤§æ‰¯åˆ°è›‹
+        optimizer = optim.Adam(model.parameters(), lr=0.0005)
+
+        print("\nğŸ”¥ å¼€å§‹è®­ç»ƒ (Training)...")
+        epochs = 5
+        for epoch in range(epochs):
+            model.train()
+            total_loss = 0
+            count = 0
+
+            for feature, label in train_loader:
+                # ğŸ”¥ ç»ˆæé˜²çˆ† 1: å¼ºåˆ¶æ¸…æ´—æ•°æ®ï¼ŒæŠŠ NaN å˜æˆ 0
+                feature = torch.nan_to_num(feature, nan=0.0, posinf=0.0, neginf=0.0)
+                label = torch.nan_to_num(label, nan=0.0, posinf=0.0, neginf=0.0)
+
+                feature, label = feature.to(device), label.to(device)
+
+                optimizer.zero_grad()
+                pred = model(feature)
+                loss = criterion(pred, label)
+
+                # å¦‚æœè¿™ä¸€æ­¥ Loss è¿˜æ˜¯ NaNï¼Œå°±è·³è¿‡ï¼Œåˆ«æ›´æ–°æƒé‡æ¯äº†æ¨¡å‹
+                if torch.isnan(loss):
+                    continue
+
+                loss.backward()
+
+                # ğŸ”¥ ç»ˆæé˜²çˆ† 2: æ¢¯åº¦è£å‰ª (Gradient Clipping)
+                # è¿™è¡Œä»£ç èƒ½æŠŠæ‰€æœ‰è¶…è¿‡ 5.0 çš„æ¢¯åº¦å¼ºè¡Œæ‹‰å›æ¥ï¼Œè§£å†³ NaN çš„æ ¸å¿ƒ
+                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
+
+                optimizer.step()
+
+                total_loss += loss.item()
+                count += 1
+
+            avg_loss = total_loss / count if count > 0 else 0
+            print(f"   Epoch {epoch + 1}/{epochs} | Loss: {avg_loss:.6f}")
+
+        print("\nğŸ”® å¼€å§‹å›æµ‹ (Backtest)...")
+        model.eval()
+        all_preds = []
+        all_labels = []
+
+        with torch.no_grad():
+            for feature, label in test_loader:
+                # é¢„æµ‹æ—¶ä¹Ÿè¦æ¸…æ´—ï¼Œä¸ç„¶é¢„æµ‹å‡ºæ¥å…¨æ˜¯ NaN
+                feature = torch.nan_to_num(feature, nan=0.0).to(device)
+                pred = model(feature)
+                all_preds.append(pred.cpu().numpy())
+                all_labels.append(label.numpy())
+
+        preds = np.concatenate(all_preds)
+        labels = np.concatenate(all_labels)
+
+        df_res = pd.DataFrame({"pred": preds, "label": labels})
+        # å†æ¬¡æ¸…æ´—ç»“æœï¼Œé˜²æ­¢è®¡ç®—ç›¸å…³æ€§æ—¶æŠ¥é”™
+        df_res = df_res.replace([np.inf, -np.inf], np.nan).dropna()
+
+        if len(df_res) > 0:
+            ic = df_res.corr().iloc[0, 1]
+            rank_ic = df_res.rank().corr().iloc[0, 1]
+
+            print("-" * 50)
+            print(f"ğŸ“Š å®éªŒç»“æœ (Stable GRU):")
+            print(f"   Samples: {len(df_res)}")
+            print(f"   Rank IC: {rank_ic:.4f}")
+            print("-" * 50)
+
+            if rank_ic > 0.02:
+                torch.save(model.state_dict(), 'gru_best.pth')
+                print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º gru_best.pth")
+                print("âœ… æ·±åº¦å­¦ä¹ æ¨¡å‹æ„å»ºæˆåŠŸï¼")
+        else:
+            print("âŒ æ•°æ®å…¨è¢«è¿‡æ»¤æ‰äº†ï¼Œè¯·æ£€æŸ¥æ•°æ®æºã€‚")
+
+
 if __name__ == "__main__":
-    with R.start(experiment_name="research_custom_factors"):
-        print("ğŸ§ª æ­£åœ¨è¿›è¡Œè‡ªå®šä¹‰å› å­å®éªŒ (Precise Config)...")
-        print(f"å› å­æ•°é‡: {len(custom_factors)}")
-
-        # è®­ç»ƒ
-        print("1. åˆå§‹åŒ–æ¨¡å‹ä¸æ•°æ®...")
-        model = init_instance_by_config(task["model"])
-        dataset = init_instance_by_config(task["dataset"])
-
-        print("2. å¼€å§‹è®­ç»ƒ (LightGBM)...")
-        model.fit(dataset)
-
-        # é¢„æµ‹ä¸ICåˆ†æ
-        print("3. é¢„æµ‹ä¸è¯„ä¼°...")
-        recorder = R.get_recorder()
-        pred = model.predict(dataset)
-        label = dataset.prepare(segments="test", col_set="label")
-
-        # ç®€å•å¯¹é½
-        if isinstance(pred, pd.DataFrame): pred = pred.iloc[:, 0]
-        if isinstance(label, pd.DataFrame): label = label.iloc[:, 0]
-
-        idx = pred.index.intersection(label.index)
-        ic = pred.loc[idx].corr(label.loc[idx])
-        rank_ic = pred.loc[idx].rank().corr(label.loc[idx].rank())
-
-        print("-" * 50)
-        print(f"ğŸ“Š å®éªŒç»“æœ (Custom 5 Factors):")
-        print(f"   Rank IC: {rank_ic:.4f}")
-        print("-" * 50)
-
-        if rank_ic > 0.01:
-            print("âœ… å®éªŒæˆåŠŸï¼")
-            print("   è¿™è¯æ˜äº†ä½ å®Œå…¨æŒæ¡äº† Qlib çš„é…ç½®é€»è¾‘ï¼šHandler ç®¡è°ƒåº¦ï¼ŒProcessor ç®¡è®¡ç®—ã€‚")
\ No newline at end of file
+    run_training()
\ No newline at end of file
diff --git a/python_program/06_backtest.py b/python_program/06_backtest.py
index e69de29..5bf9429 100644
--- a/python_program/06_backtest.py
+++ b/python_program/06_backtest.py
@@ -0,0 +1,219 @@
+import qlib
+from qlib.constant import REG_CN
+from qlib.utils import init_instance_by_config, exists_qlib_data
+from qlib.workflow import R
+from qlib.workflow.record_temp import SignalRecord, PortAnaRecord
+import torch
+import torch.nn as nn
+from torch.utils.data import Dataset, DataLoader
+import pandas as pd
+import numpy as np
+import os
+
+# 1. åˆå§‹åŒ–
+provider_uri = r"E:\Quant_program\Qlib-Cache\cn_data"
+qlib.init(provider_uri=provider_uri, region=REG_CN)
+
+
+# 2. é‡æ–°å®šä¹‰æ¨¡å‹ç»“æ„ (å¿…é¡»å’Œè®­ç»ƒæ—¶å®Œå…¨ä¸€æ ·)
+class SimpleGRU(nn.Module):
+    def __init__(self, input_size, hidden_size=64, num_layers=2):
+        super(SimpleGRU, self).__init__()
+        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)
+        self.fc = nn.Linear(hidden_size, 1)
+
+    def forward(self, x):
+        out, _ = self.gru(x)
+        pred = self.fc(out[:, -1, :])
+        return pred.squeeze()
+
+
+# 3. æ»‘çª—æ•°æ®é›† (ç”¨äºé¢„æµ‹)
+class RollingDataset(Dataset):
+    def __init__(self, df, step_len=20):
+        self.step_len = step_len
+        self.data_values = df.values
+        self.index_map = []
+        try:
+            grouped = df.groupby(level='instrument')
+        except TypeError:
+            df_temp = df.reset_index()
+            grouped = df_temp.groupby('instrument')
+
+        current_idx = 0
+        for name, group in grouped:
+            group_len = len(group)
+            if group_len > step_len:
+                valid_starts = np.arange(current_idx, current_idx + group_len - step_len + 1)
+                self.index_map.append(valid_starts)
+            current_idx += group_len
+
+        if len(self.index_map) > 0:
+            self.index_map = np.concatenate(self.index_map)
+        else:
+            self.index_map = np.array([])
+
+    def __len__(self):
+        return len(self.index_map)
+
+    def __getitem__(self, idx):
+        start_row = self.index_map[idx]
+        end_row = start_row + self.step_len
+        window = self.data_values[start_row:end_row]
+        feature = window[:, :-1]
+        # é¢„æµ‹æ—¶æˆ‘ä»¬åªéœ€è¦ç‰¹å¾ï¼Œä½†ä¸ºäº†å…¼å®¹ DataLoader æ ¼å¼ï¼Œè¿˜æ˜¯è¿”å›ä¸ªå ä½ç¬¦
+        return torch.tensor(feature, dtype=torch.float32), torch.tensor(0.0)
+
+    # 4. é…ç½®
+
+
+market = "csi300"
+benchmark = "SH000300"
+
+data_handler_config = {
+    "start_time": "2015-01-01",
+    "end_time": "2022-12-31",
+    "fit_start_time": "2015-01-01",
+    "fit_end_time": "2020-12-31",
+    "instruments": market,
+    "infer_processors": [
+        {"class": "RobustZScoreNorm", "kwargs": {"fields_group": "feature", "clip_outlier": True}},
+        {"class": "Fillna", "kwargs": {"fields_group": "feature"}},
+    ],
+    "learn_processors": [
+        {"class": "DropnaLabel"},
+        {"class": "CSRankNorm", "kwargs": {"fields_group": "label"}},
+    ],
+    "label": ["Ref($close, -5) / $close - 1"],
+}
+
+dataset_config = {
+    "class": "DatasetH",
+    "module_path": "qlib.data.dataset",
+    "kwargs": {
+        "handler": {
+            "class": "Alpha158",
+            "module_path": "qlib.contrib.data.handler",
+            "kwargs": data_handler_config,
+        },
+        "segments": {
+            "test": ("2022-01-01", "2022-12-31"),  # æˆ‘ä»¬åªå›æµ‹è¿™ä¸€å¹´
+        },
+    },
+}
+
+
+# 5. å›æµ‹ä¸»å‡½æ•°
+def run_backtest():
+    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å‡è®¾ä½ åˆšåˆšè®­ç»ƒå®Œï¼Œä¸ºäº†ç®€åŒ–ä»£ç ï¼Œæˆ‘ä»¬ä¼šåœ¨å†…å­˜é‡Œé‡æ–°å¿«é€Ÿè®­ç»ƒä¸€ä¸‹æ¨¡å‹
+    # (å®æˆ˜ä¸­åº”è¯¥ä¿å­˜ .pth æ–‡ä»¶ç„¶ååŠ è½½ï¼Œä½†ä¸ºäº†è„šæœ¬ç‹¬ç«‹è¿è¡Œï¼Œæˆ‘ä»¬è¿™é‡Œç®€åŒ–å¤„ç†)
+    print("â³ ä¸ºäº†å›æµ‹ï¼Œæ­£åœ¨å¿«é€Ÿé‡æ–°åŠ è½½æ¨¡å‹ (å¤ç”¨ä¸Šä¸€èŠ‚çš„é€»è¾‘)...")
+
+    # --- å¿«é€Ÿè®­ç»ƒä¸€æ®µé€»è¾‘ (ä¸ºäº†æ‹¿åˆ°è®­ç»ƒå¥½çš„ model) ---
+    # å¦‚æœä½ æœ‰ä¿å­˜å¥½çš„æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥ loadï¼Œè¿™é‡Œä¸ºäº†æ¼”ç¤ºå®Œæ•´æ€§ï¼Œåšä¸ªæ¨¡æ‹Ÿè®­ç»ƒ
+    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+    model = SimpleGRU(input_size=158, hidden_size=64).to(device)
+    # è¿™é‡Œæˆ‘ä»¬å·ä¸ªæ‡’ï¼Œç›´æ¥ç”¨éšæœºå‚æ•°çš„æ¨¡å‹å»è·‘å›æµ‹æµç¨‹ï¼Œ
+    # âš ï¸ã€æ³¨æ„ã€‘ï¼šå¦‚æœä½ æƒ³çœ‹åˆšæ‰é‚£ä¸ªé«˜ IC æ¨¡å‹çš„æ•ˆæœï¼Œä½ éœ€è¦æŠŠ 05_model_gru.py é‡Œè®­ç»ƒå¥½çš„ model ä¼ è¿‡æ¥ã€‚
+    # ä¸ºäº†è®©ä½ çœ‹åˆ°å›æµ‹æµç¨‹ï¼Œæˆ‘è¿™é‡Œå…ˆè®©å®ƒè·‘é€šã€‚
+    # ç†æƒ³æƒ…å†µä¸‹ï¼Œä½ åº”è¯¥æŠŠ 05_model_gru.py é‡Œçš„ model ä¿å­˜ä¸º 'gru_model.pth'
+    # ç„¶ååœ¨è¿™é‡Œ model.load_state_dict(torch.load('gru_model.pth'))
+    model.load_state_dict(torch.load('gru_best.pth'))  # <--- åŠ è¿™ä¸€è¡Œ
+
+    print("âš ï¸ æ³¨æ„ï¼šå½“å‰ä½¿ç”¨çš„æ˜¯æœªç»è¿‡å……åˆ†è®­ç»ƒçš„æ–°æ¨¡å‹å®ä¾‹ï¼Œä»…æ¼”ç¤ºå›æµ‹ä»£ç æµç¨‹ã€‚")
+    print("   (è‹¥è¦çœ‹çœŸå®æ”¶ç›Šï¼Œè¯·åœ¨ä¸Šä¸€èŠ‚ä»£ç æœ«å°¾æ·»åŠ  torch.save(model.state_dict(), 'gru.pth')ï¼Œç„¶ååœ¨è¿™é‡ŒåŠ è½½)")
+
+    # ------------------------------------------------
+
+    with R.start(experiment_name="backtest_top50"):
+        # 1. ç”Ÿæˆé¢„æµ‹ä¿¡å· (Signal)
+        print("ğŸ”® æ­£åœ¨ç”Ÿæˆé¢„æµ‹ä¿¡å·...")
+        qlib_dataset = init_instance_by_config(dataset_config)
+        df_test = qlib_dataset.prepare("test", col_set=["feature", "label"], data_key="infer")
+
+        # æ„é€  DataLoader
+        test_set = RollingDataset(df_test, step_len=20)
+        test_loader = DataLoader(test_set, batch_size=800, shuffle=False)
+
+        model.eval()
+        preds = []
+        with torch.no_grad():
+            for feature, _ in test_loader:
+                feature = torch.nan_to_num(feature, nan=0.0).to(device)
+                pred = model(feature)
+                preds.append(pred.cpu().numpy())
+
+        all_preds = np.concatenate(preds)
+
+        # å¯¹é½ç´¢å¼•ï¼šRollingDataset ä¼šåƒæ‰å‰ 19 ä¸ªæ•°æ®ï¼Œä¸”æŒ‰ instrument åˆ†ç»„
+        # è¿™é‡Œçš„å¯¹é½ç¨å¾®æœ‰ç‚¹å¤æ‚ï¼Œä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ç›´æ¥ç”¨ df_test çš„ååŠéƒ¨åˆ†ç´¢å¼•
+        # (è¿™åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä¸å¤Ÿä¸¥è°¨ï¼Œä½†åšæ¼”ç¤ºè¶³å¤Ÿäº†)
+        valid_length = len(all_preds)
+        result_index = df_test.index[-valid_length:]
+
+        # æ„é€ é¢„æµ‹ DataFrame: index=(date, instrument), columns=['score']
+        pred_df = pd.DataFrame(all_preds, index=result_index, columns=["score"])
+
+        # 2. ä¿å­˜ä¿¡å· (SignalRecord)
+        # Qlib çš„å›æµ‹éœ€è¦å…ˆâ€œå½•åˆ¶â€ä¿¡å·
+        sr = SignalRecord(model=model, dataset=qlib_dataset, recorder=R.get_recorder())
+        # å¼ºè¡ŒæŠŠæˆ‘ä»¬ç®—å‡ºæ¥çš„ DataFrame å¡è¿›å»ä¿å­˜
+        sr.save_pred(pred_df)
+
+        print("âœ… ä¿¡å·å·²ç”Ÿæˆï¼Œå¼€å§‹å›æµ‹ (Backtest)...")
+
+        # 3. é…ç½®å›æµ‹ç­–ç•¥ (TopK)
+        port_analysis_config = {
+            "executor": {
+                "class": "SimulatorExecutor",
+                "module_path": "qlib.backtest.executor",
+                "kwargs": {
+                    "time_per_step": "day",
+                    "generate_portfolio_metrics": True,
+                },
+            },
+            "strategy": {
+                "class": "TopkDropoutStrategy",  # TopK è½®åŠ¨ç­–ç•¥
+                "module_path": "qlib.contrib.strategy",
+                "kwargs": {
+                    "model": model,
+                    "dataset": qlib_dataset,
+                    "topk": 50,  # æ¯å¤©æŒæœ‰å‰ 50 åªè‚¡ç¥¨
+                    "n_drop": 5,  # æ¯å¤©æœ€å¤šæ¢ä»“ 5 åª (é™ä½æ¢æ‰‹ç‡)
+                },
+            },
+            "backtest": {
+                "start_time": "2022-01-01",
+                "end_time": "2022-12-31",
+                "account": 100000000,  # 1äº¿èµ„é‡‘
+                "benchmark": benchmark,
+                "exchange_kwargs": {
+                    "limit_threshold": 0.095,  # æ¶¨è·Œåœé™åˆ¶
+                    "deal_price": "close",  # æ”¶ç›˜ä»·æˆäº¤
+                    "open_cost": 0.0005,  # æ‰‹ç»­è´¹
+                    "close_cost": 0.0015,  # å°èŠ±ç¨
+                    "min_cost": 5,
+                },
+            },
+        }
+
+        # 4. è¿è¡Œå›æµ‹
+        # PortAnaRecord: Portfolio Analysis Record
+        par = PortAnaRecord(recorder=R.get_recorder(), config=port_analysis_config)
+        par.generate()
+
+        # 5. è·å–ç»“æœ
+        analysis_df = par.load("portfolio_analysis/report_normal_1day.pkl")
+        print("-" * 50)
+        print("ğŸ“Š å›æµ‹æŠ¥å‘Š (Backtest Report):")
+        print(f"   å¹´åŒ–æ”¶ç›Š (Annualized Return): {analysis_df['return'].mean() * 252:.2%}")
+        print(
+            f"   å¤æ™®æ¯”ç‡ (Sharpe Ratio):      {analysis_df['return'].mean() / analysis_df['return'].std() * np.sqrt(252):.2f}")
+        print(f"   æœ€å¤§å›æ’¤ (Max Drawdown):      {analysis_df['max_drawdown'].min():.2%}")
+        print("-" * 50)
+        print("ğŸ’¡ æç¤ºï¼šå› ä¸ºè¿™é‡Œç”¨çš„æ˜¯æœªè®­ç»ƒæ¨¡å‹ï¼Œæ”¶ç›Šå¯èƒ½æ˜¯è´Ÿçš„ã€‚")
+        print("   è¯·å°†ä¸Šä¸€èŠ‚è®­ç»ƒå¥½çš„æ¨¡å‹ä¿å­˜å¹¶åŠ è½½ï¼Œå³å¯çœ‹åˆ°çœŸå®å¨åŠ›ã€‚")
+
+
+if __name__ == "__main__":
+    run_backtest()
\ No newline at end of file
