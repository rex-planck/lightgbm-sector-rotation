import qlib
from qlib.constant import REG_CN
from qlib.utils import init_instance_by_config
from qlib.workflow import R
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import os

# 1. åˆå§‹åŒ–
provider_uri = r"E:\Quant_program\Qlib-Cache\cn_data"
qlib.init(provider_uri=provider_uri, region=REG_CN)


# 2. PyTorch GRU æ¨¡å‹ (å¢åŠ ä¸€ç‚¹ç¨³å®šæ€§)
class SimpleGRU(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2):
        super(SimpleGRU, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)  # åŠ ç‚¹ Dropout
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        out, _ = self.gru(x)
        pred = self.fc(out[:, -1, :])
        return pred.squeeze()


# 3. æ»‘çª— Dataset (ä¿æŒä¸å˜ï¼Œé€»è¾‘æ²¡é—®é¢˜)
class RollingDataset(Dataset):
    def __init__(self, df, step_len=20):
        self.step_len = step_len
        self.data_values = df.values
        self.index_map = []

        # æŒ‰ instrument åˆ†ç»„è®¡ç®—åˆ‡ç‰‡ç´¢å¼•
        # è¿™é‡Œä¸ºäº†é˜²æ­¢ df index ä¸è§„èŒƒï¼Œæˆ‘ä»¬å°è¯• reset_index å† groupby
        # ä½† Qlib æ•°æ®é€šå¸¸æ˜¯ MultiIndexï¼Œç›´æ¥ groupby(level='instrument') å³å¯
        try:
            grouped = df.groupby(level='instrument')
        except TypeError:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœç´¢å¼•æœ‰é—®é¢˜ï¼Œå¼ºåˆ¶é‡ç½®
            df_temp = df.reset_index()
            grouped = df_temp.groupby('instrument')

        current_idx = 0
        for name, group in grouped:
            group_len = len(group)
            if group_len > step_len:
                valid_starts = np.arange(current_idx, current_idx + group_len - step_len + 1)
                self.index_map.append(valid_starts)
            current_idx += group_len

        if len(self.index_map) > 0:
            self.index_map = np.concatenate(self.index_map)
        else:
            self.index_map = np.array([])

    def __len__(self):
        return len(self.index_map)

    def __getitem__(self, idx):
        start_row = self.index_map[idx]
        end_row = start_row + self.step_len
        window = self.data_values[start_row:end_row]

        feature = window[:, :-1]
        label = window[-1, -1]

        return torch.tensor(feature, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)


# 4. é…ç½®
market = "csi300"
data_handler_config = {
    "start_time": "2015-01-01",
    "end_time": "2022-12-31",
    "fit_start_time": "2015-01-01",
    "fit_end_time": "2020-12-31",
    "instruments": market,
    "infer_processors": [
        {"class": "RobustZScoreNorm", "kwargs": {"fields_group": "feature", "clip_outlier": True}},
        {"class": "Fillna", "kwargs": {"fields_group": "feature"}},
    ],
    "learn_processors": [
        {"class": "DropnaLabel"},
        {"class": "CSRankNorm", "kwargs": {"fields_group": "label"}},
    ],
    "label": ["Ref($close, -5) / $close - 1"],
}

dataset_config = {
    "class": "DatasetH",
    "module_path": "qlib.data.dataset",
    "kwargs": {
        "handler": {
            "class": "Alpha158",
            "module_path": "qlib.contrib.data.handler",
            "kwargs": data_handler_config,
        },
        "segments": {
            "train": ("2015-01-01", "2020-12-31"),
            "valid": ("2021-01-01", "2021-12-31"),
            "test": ("2022-01-01", "2022-12-31"),
        },
    },
}


def run_training():
    with R.start(experiment_name="gru_rolling_window_stable"):
        print("ğŸ› ï¸ åˆå§‹åŒ–æ•°æ®...")
        qlib_dataset = init_instance_by_config(dataset_config)
        df_train = qlib_dataset.prepare("train", col_set=["feature", "label"], data_key="infer")
        df_valid = qlib_dataset.prepare("valid", col_set=["feature", "label"], data_key="infer")
        df_test = qlib_dataset.prepare("test", col_set=["feature", "label"], data_key="infer")

        print(f"   Train shape: {df_train.shape}")

        print("ğŸ”„ æ„å»ºæ•°æ®é›†...")
        train_set = RollingDataset(df_train, step_len=20)
        valid_set = RollingDataset(df_valid, step_len=20)
        test_set = RollingDataset(df_test, step_len=20)

        # å¢å¤§ Batch Size ä¹Ÿèƒ½ç¨å¾®ç¨³ä¸€ç‚¹
        train_loader = DataLoader(train_set, batch_size=1024, shuffle=True, num_workers=0)
        valid_loader = DataLoader(valid_set, batch_size=1024, shuffle=False, num_workers=0)
        test_loader = DataLoader(test_set, batch_size=1024, shuffle=False, num_workers=0)

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")

        model = SimpleGRU(input_size=158, hidden_size=64).to(device)
        criterion = nn.MSELoss()
        # ğŸ”¥ é™ä½å­¦ä¹ ç‡ï¼Œé˜²æ­¢æ­¥å­è¿ˆå¤ªå¤§æ‰¯åˆ°è›‹
        optimizer = optim.Adam(model.parameters(), lr=0.0005)

        print("\nğŸ”¥ å¼€å§‹è®­ç»ƒ (Training)...")
        epochs = 5
        for epoch in range(epochs):
            model.train()
            total_loss = 0
            count = 0

            for feature, label in train_loader:
                # ğŸ”¥ ç»ˆæé˜²çˆ† 1: å¼ºåˆ¶æ¸…æ´—æ•°æ®ï¼ŒæŠŠ NaN å˜æˆ 0
                feature = torch.nan_to_num(feature, nan=0.0, posinf=0.0, neginf=0.0)
                label = torch.nan_to_num(label, nan=0.0, posinf=0.0, neginf=0.0)

                feature, label = feature.to(device), label.to(device)

                optimizer.zero_grad()
                pred = model(feature)
                loss = criterion(pred, label)

                # å¦‚æœè¿™ä¸€æ­¥ Loss è¿˜æ˜¯ NaNï¼Œå°±è·³è¿‡ï¼Œåˆ«æ›´æ–°æƒé‡æ¯äº†æ¨¡å‹
                if torch.isnan(loss):
                    continue

                loss.backward()

                # ğŸ”¥ ç»ˆæé˜²çˆ† 2: æ¢¯åº¦è£å‰ª (Gradient Clipping)
                # è¿™è¡Œä»£ç èƒ½æŠŠæ‰€æœ‰è¶…è¿‡ 5.0 çš„æ¢¯åº¦å¼ºè¡Œæ‹‰å›æ¥ï¼Œè§£å†³ NaN çš„æ ¸å¿ƒ
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)

                optimizer.step()

                total_loss += loss.item()
                count += 1

            avg_loss = total_loss / count if count > 0 else 0
            print(f"   Epoch {epoch + 1}/{epochs} | Loss: {avg_loss:.6f}")

        print("\nğŸ”® å¼€å§‹å›æµ‹ (Backtest)...")
        model.eval()
        all_preds = []
        all_labels = []

        with torch.no_grad():
            for feature, label in test_loader:
                # é¢„æµ‹æ—¶ä¹Ÿè¦æ¸…æ´—ï¼Œä¸ç„¶é¢„æµ‹å‡ºæ¥å…¨æ˜¯ NaN
                feature = torch.nan_to_num(feature, nan=0.0).to(device)
                pred = model(feature)
                all_preds.append(pred.cpu().numpy())
                all_labels.append(label.numpy())

        preds = np.concatenate(all_preds)
        labels = np.concatenate(all_labels)

        df_res = pd.DataFrame({"pred": preds, "label": labels})
        # å†æ¬¡æ¸…æ´—ç»“æœï¼Œé˜²æ­¢è®¡ç®—ç›¸å…³æ€§æ—¶æŠ¥é”™
        df_res = df_res.replace([np.inf, -np.inf], np.nan).dropna()

        if len(df_res) > 0:
            ic = df_res.corr().iloc[0, 1]
            rank_ic = df_res.rank().corr().iloc[0, 1]

            print("-" * 50)
            print(f"ğŸ“Š å®éªŒç»“æœ (Stable GRU):")
            print(f"   Samples: {len(df_res)}")
            print(f"   Rank IC: {rank_ic:.4f}")
            print("-" * 50)

            if rank_ic > 0.02:
                torch.save(model.state_dict(), 'gru_best.pth')
                print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º gru_best.pth")
                print("âœ… æ·±åº¦å­¦ä¹ æ¨¡å‹æ„å»ºæˆåŠŸï¼")
        else:
            print("âŒ æ•°æ®å…¨è¢«è¿‡æ»¤æ‰äº†ï¼Œè¯·æ£€æŸ¥æ•°æ®æºã€‚")


if __name__ == "__main__":
    run_training()