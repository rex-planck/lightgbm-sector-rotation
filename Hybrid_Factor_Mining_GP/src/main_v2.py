"""
é¡¹ç›®ä¸»å…¥å£ V2
æ•´åˆä¼˜åŒ–åçš„æ‰€æœ‰æ¨¡å—
"""
import argparse
import os
import sys
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from config import TUSHARE_TOKEN, DB_PATH, OUTPUT_DIR

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(os.path.join(OUTPUT_DIR, 'pipeline.log'), encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


def check_config():
    """æ£€æŸ¥é…ç½®"""
    if not TUSHARE_TOKEN:
        logger.error("âŒ è¯·åœ¨ config.py ä¸­è®¾ç½® TUSHARE_TOKEN")
        return False
    logger.info(f"âœ… Token é…ç½®æ­£å¸¸: {TUSHARE_TOKEN[:10]}...")
    return True


def step1_fetch_data(force: bool = False):
    """æ­¥éª¤1: è·å–æ•°æ®"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“¥ æ­¥éª¤ 1/4: è·å– Tushare æ•°æ®")
    logger.info("=" * 60)
    
    from data_fetcher_optimized import TushareDataFetcherOptimized
    
    fetcher = TushareDataFetcherOptimized()
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è·å–
    if not force and os.path.exists(DB_PATH):
        import sqlite3
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM daily_price")
        count = cursor.fetchone()[0]
        conn.close()
        
        if count > 0:
            logger.info(f"   æ•°æ®åº“å·²æœ‰ {count} æ¡æ—¥çº¿æ•°æ®")
            response = input("   æ˜¯å¦é‡æ–°è·å–ï¼Ÿ(y/N): ").strip().lower()
            if response != 'y':
                logger.info("   è·³è¿‡æ•°æ®è·å–")
                return
    
    # è·å–æ•°æ®
    fetcher.fetch_stock_basic()
    stock_list = fetcher.fetch_index_components()
    
    if stock_list:
        fetcher.fetch_all_data_by_date(stock_list)
    
    logger.info("âœ… æ•°æ®è·å–å®Œæˆ")


def step2_prepare_data():
    """æ­¥éª¤2: æ•°æ®é¢„å¤„ç†"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ”§ æ­¥éª¤ 2/4: æ•°æ®é¢„å¤„ç†")
    logger.info("=" * 60)
    
    from data_loader import DataLoader
    
    loader = DataLoader()
    df_raw = loader.load_all_data()
    df_features = loader.prepare_features(df_raw)
    df_labeled = loader.prepare_labels(df_features)
    loader.close()
    
    logger.info(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ: {len(df_labeled)} æ¡æœ‰æ•ˆæ•°æ®")
    return df_labeled


def step3_mine_factors():
    """æ­¥éª¤3: GP å› å­æŒ–æ˜"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ§¬ æ­¥éª¤ 3/4: GP å› å­æŒ–æ˜")
    logger.info("=" * 60)
    
    from data_loader import DataLoader
    from gp_factor_mining_v2 import GPFactorMinerV2
    from config import TRAIN_END, GP_CONFIG
    
    # åŠ è½½æ•°æ®
    loader = DataLoader()
    df_raw = loader.load_all_data()
    loader.close()
    
    # å‡†å¤‡ç‰¹å¾
    miner = GPFactorMinerV2()
    df_features, feature_cols = miner.prepare_features(df_raw)
    
    # è¿™é‡Œéœ€è¦é‡æ–°åŠ è½½ DataLoader æ¥å‡†å¤‡æ ‡ç­¾
    loader = DataLoader()
    df_labeled = loader.prepare_labels(df_features)
    loader.close()
    
    # åªä½¿ç”¨è®­ç»ƒé›†
    train_df = df_labeled[df_labeled['trade_date'] <= TRAIN_END]
    
    logger.info(f"   è®­ç»ƒé›†æ ·æœ¬: {len(train_df)}")
    logger.info(f"   ä½¿ç”¨ç‰¹å¾: {feature_cols}")
    
    # æŒ–æ˜å› å­
    programs = miner.mine_factors_symbolic_transformer(
        train_df, feature_cols, n_factors=GP_CONFIG['n_factors']
    )
    
    # éªŒè¯å› å­
    df_results = miner.validate_factors(
        programs, train_df, feature_cols, min_ic=GP_CONFIG['min_ic_threshold']
    )
    
    # é€‰æ‹©å¤šæ ·åŒ–å› å­
    df_results = miner.select_diverse_factors(
        df_results, train_df, feature_cols,
        top_n=30, max_corr=GP_CONFIG['max_correlation']
    )
    
    # ä¿å­˜ç»“æœ
    miner.save_factors(df_results)
    
    # æ‰“å°ç»“æœ
    valid_count = df_results['valid'].sum() if 'valid' in df_results.columns else len(df_results)
    selected_count = df_results['selected'].sum() if 'selected' in df_results.columns else 0
    
    logger.info(f"âœ… å› å­æŒ–æ˜å®Œæˆ: {valid_count} ä¸ªæœ‰æ•ˆï¼Œ{selected_count} ä¸ªè¢«é€‰ä¸­")
    
    # æ‰“å° Top 5
    if 'ir' in df_results.columns:
        logger.info("\nğŸ† Top 5 å› å­:")
        for idx, row in df_results.head(5).iterrows():
            logger.info(f"   [{idx+1}] IR={row.get('ir', 0):.3f}, IC={row.get('ic_mean', 0):.4f}")


def step4_train_model():
    """æ­¥éª¤4: è®­ç»ƒä¸¤é˜¶æ®µæ¨¡å‹"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸš€ æ­¥éª¤ 4/4: ä¸¤é˜¶æ®µæ¨¡å‹è®­ç»ƒ (GP + GRU)")
    logger.info("=" * 60)
    
    from data_loader import DataLoader
    from two_stage_model_v2 import TwoStageModelV2
    from config import TRAIN_END, VALID_END
    
    # åŠ è½½æ•°æ®
    loader = DataLoader()
    df_raw = loader.load_all_data()
    df_features = loader.prepare_features(df_raw)
    df_labeled = loader.prepare_labels(df_features)
    loader.close()
    
    # åˆ’åˆ†æ•°æ®é›†
    train_df = df_labeled[df_labeled['trade_date'] <= TRAIN_END]
    valid_df = df_labeled[(df_labeled['trade_date'] > TRAIN_END) & 
                           (df_labeled['trade_date'] <= VALID_END)]
    test_df = df_labeled[df_labeled['trade_date'] > VALID_END]
    
    logger.info(f"   è®­ç»ƒ: {len(train_df)}, éªŒè¯: {len(valid_df)}, æµ‹è¯•: {len(test_df)}")
    
    # åˆ›å»ºæ¨¡å‹
    model = TwoStageModelV2()
    
    # åŠ è½½/è®¡ç®— GP å› å­
    base_features = ['open', 'high', 'low', 'close', 'vol', 'ret_1d', 'ret_5d']
    base_features = [c for c in base_features if c in train_df.columns]
    
    if not model.load_gp_factors():
        logger.warning("âš ï¸ æœªæ‰¾åˆ° GP å› å­ï¼Œå°†åªä½¿ç”¨ Alpha å› å­")
    
    # è®¡ç®—å› å­
    logger.info("ğŸ”§ è®¡ç®— GP å› å­...")
    train_df = model.compute_gp_factors(train_df, base_features)
    valid_df = model.compute_gp_factors(valid_df, base_features)
    test_df = model.compute_gp_factors(test_df, base_features)
    
    # è®­ç»ƒ GRU
    model.train(train_df, valid_df)
    
    # æµ‹è¯•é›†è¯„ä¼°
    logger.info("\nğŸ“Š æµ‹è¯•é›†è¯„ä¼°...")
    model.load_model("gru_best_v2.pth")
    test_result = model.predict(test_df)
    
    # è®¡ç®—æµ‹è¯•é›† IC
    test_mask = test_result['pred'].notna()
    if test_mask.sum() > 100:
        import numpy as np
        test_ic = np.corrcoef(
            test_result.loc[test_mask, 'pred'].rank(),
            test_result.loc[test_mask, 'label'].rank()
        )[0, 1]
        logger.info(f"   æµ‹è¯•é›† Rank IC: {test_ic:.4f}")
    
    logger.info("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")


def run_full_pipeline(force_fetch: bool = False):
    """è¿è¡Œå®Œæ•´æµç¨‹"""
    start_time = datetime.now()
    
    if not check_config():
        return
    
    try:
        step1_fetch_data(force=force_fetch)
    except Exception as e:
        logger.error(f"âŒ æ•°æ®è·å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    try:
        step2_prepare_data()
    except Exception as e:
        logger.error(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    try:
        step3_mine_factors()
    except Exception as e:
        logger.error(f"âŒ å› å­æŒ–æ˜å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    try:
        step4_train_model()
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    elapsed = (datetime.now() - start_time).total_seconds() / 60
    logger.info("\n" + "=" * 60)
    logger.info(f"ğŸ‰ å…¨æµç¨‹å®Œæˆï¼è€—æ—¶: {elapsed:.1f} åˆ†é’Ÿ")
    logger.info("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="åŸºäº GPlearn çš„'æœºç†+æ•°æ®'æ··åˆå› å­æŒ–æ˜ç³»ç»Ÿ V2"
    )
    parser.add_argument(
        "--step",
        choices=["all", "fetch", "prepare", "mine", "train"],
        default="all",
        help="é€‰æ‹©è¿è¡Œæ­¥éª¤"
    )
    parser.add_argument(
        "--force-fetch",
        action="store_true",
        help="å¼ºåˆ¶é‡æ–°è·å–æ•°æ®"
    )
    
    args = parser.parse_args()
    
    if args.step == "all":
        run_full_pipeline(force_fetch=args.force_fetch)
    elif args.step == "fetch":
        if check_config():
            step1_fetch_data(force=args.force_fetch)
    elif args.step == "prepare":
        step2_prepare_data()
    elif args.step == "mine":
        step3_mine_factors()
    elif args.step == "train":
        step4_train_model()


if __name__ == "__main__":
    main()
