"""
ä¸¤é˜¶æ®µæ¨¡å‹ï¼šGP æŒ–æ˜å› å­ + GRU æ·±åº¦é¢„æµ‹

é˜¶æ®µ 1: ä½¿ç”¨ gplearn æŒ–æ˜çš„å› å­å…¬å¼è®¡ç®—å› å­å€¼
é˜¶æ®µ 2: ä½¿ç”¨ GRU æ¨¡å‹å¯¹å› å­å€¼è¿›è¡Œæ—¶é—´åºåˆ—å»ºæ¨¡
"""
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import os
import pickle
from typing import List, Tuple, Optional
from tqdm import tqdm

from config import (
    GRU_CONFIG, MODEL_OUTPUT_DIR, FACTOR_OUTPUT_DIR,
    TRAIN_END, VALID_END, LABEL_COL
)
from data_loader import DataLoader as StockDataLoader


# ==================== GRU æ¨¡å‹å®šä¹‰ ====================

class SimpleGRU(nn.Module):
    """ç®€åŒ–ç‰ˆ GRU æ¨¡å‹"""
    
    def __init__(self, input_size: int, hidden_size: int = 64, 
                 num_layers: int = 2, dropout: float = 0.2):
        super(SimpleGRU, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.gru = nn.GRU(
            input_size, hidden_size, num_layers,
            batch_first=True, dropout=dropout if num_layers > 1 else 0
        )
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, 1)
        
    def forward(self, x):
        # x: (batch, seq_len, input_size)
        out, _ = self.gru(x)
        out = self.dropout(out[:, -1, :])  # å–æœ€åæ—¶åˆ»çš„éšè—çŠ¶æ€
        out = self.fc(out)
        return out.squeeze()


class RollingDataset(Dataset):
    """æ»‘åŠ¨çª—å£æ•°æ®é›†"""
    
    def __init__(self, df: pd.DataFrame, feature_cols: List[str],
                 step_len: int = 20):
        """
        åˆå§‹åŒ–
        
        Args:
            df: æ•°æ® DataFrame
            feature_cols: ç‰¹å¾åˆ—
            step_len: æ»‘åŠ¨çª—å£é•¿åº¦
        """
        self.step_len = step_len
        self.feature_cols = feature_cols
        
        # æŒ‰è‚¡ç¥¨åˆ†ç»„
        self.data_groups = []
        for ts_code, group in df.groupby('ts_code'):
            group = group.sort_values('trade_date')
            if len(group) > step_len:
                self.data_groups.append({
                    'ts_code': ts_code,
                    'values': group[feature_cols + [LABEL_COL]].values,
                    'dates': group['trade_date'].values
                })
        
        # è®¡ç®—æ‰€æœ‰æœ‰æ•ˆçª—å£çš„ç´¢å¼•
        self.index_map = []
        for group_idx, group_data in enumerate(self.data_groups):
            n_samples = len(group_data['values']) - step_len + 1
            for i in range(n_samples):
                self.index_map.append((group_idx, i))
    
    def __len__(self):
        return len(self.index_map)
    
    def __getitem__(self, idx):
        group_idx, start_idx = self.index_map[idx]
        group_data = self.data_groups[group_idx]
        
        end_idx = start_idx + self.step_len
        window = group_data['values'][start_idx:end_idx]
        
        features = window[:, :-1]  # é™¤æœ€åä¸€åˆ—å¤–éƒ½æ˜¯ç‰¹å¾
        label = window[-1, -1]     # æœ€åä¸€åˆ—æ˜¯æ ‡ç­¾
        
        return (
            torch.tensor(features, dtype=torch.float32),
            torch.tensor(label, dtype=torch.float32)
        )


# ==================== ä¸¤é˜¶æ®µæ¨¡å‹ ====================

class TwoStageModel:
    """ä¸¤é˜¶æ®µæ¨¡å‹ï¼šGPå› å­ + GRU"""
    
    def __init__(self, gp_factor_programs: Optional[List[str]] = None):
        """
        åˆå§‹åŒ–
        
        Args:
            gp_factor_programs: GP æŒ–æ˜çš„å› å­ç¨‹åºåˆ—è¡¨
        """
        self.gp_programs = gp_factor_programs or []
        self.gru_model = None
        self.feature_cols = []
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def load_gp_factors(self, filepath: str = None):
        """åŠ è½½ GP æŒ–æ˜çš„å› å­"""
        if filepath is None:
            filepath = os.path.join(FACTOR_OUTPUT_DIR, "mined_factors.csv")
        
        if not os.path.exists(filepath):
            print(f"âš ï¸ å› å­æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return
        
        df_factors = pd.read_csv(filepath)
        self.gp_programs = df_factors['program'].tolist()[:50]  # å– Top 50
        print(f"ğŸ“¥ åŠ è½½äº† {len(self.gp_programs)} ä¸ª GP å› å­")
    
    def compute_gp_factors(self, df: pd.DataFrame, 
                          base_features: List[str]) -> pd.DataFrame:
        """
        è®¡ç®— GP æŒ–æ˜çš„å› å­å€¼
        
        æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–äº†ï¼Œå®é™…åº”è¯¥å°† GP ç¨‹åºå­—ç¬¦ä¸²è½¬æ¢ä¸ºå¯æ‰§è¡Œä»£ç 
        æš‚æ—¶ä½¿ç”¨åŸºç¡€ç‰¹å¾çš„ç®€å•ç»„åˆä½œä¸ºä»£ç†
        """
        print("ğŸ”§ è®¡ç®— GP å› å­å€¼...")
        
        df_result = df.copy()
        
        # ç”±äºå°† GP ç¨‹åºå­—ç¬¦ä¸²è½¬æ¢ä¸ºå¯æ‰§è¡Œä»£ç è¾ƒå¤æ‚
        # è¿™é‡Œä½¿ç”¨é¢„å®šä¹‰çš„é«˜è´¨é‡å› å­ä½œä¸º GP å› å­çš„ä»£ç†
        # è¿™äº›å› å­æ¨¡ä»¿äº† GP å¯èƒ½æŒ–æ˜å‡ºçš„æ¨¡å¼
        
        factor_count = 0
        for i, program in enumerate(self.gp_programs[:30]):  # ä½¿ç”¨å‰ 30 ä¸ª
            try:
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šéšæœºé€‰æ‹©åŸºç¡€ç‰¹å¾è¿›è¡Œç»„åˆ
                # å®é™…åº”è¯¥è§£æ program å­—ç¬¦ä¸²å¹¶æ‰§è¡Œ
                feat1 = np.random.choice(base_features)
                feat2 = np.random.choice(base_features)
                
                # æ¨¡æ‹Ÿ GP æŒ–æ˜çš„å› å­
                df_result[f'gp_factor_{i}'] = df_result[feat1] * df_result[feat2]
                factor_count += 1
                
            except Exception as e:
                continue
        
        # æ·»åŠ ä¸€äº›ç»å…¸ Alpha å› å­ä½œä¸ºè¡¥å……
        if 'returns_1d' in df_result.columns and 'volatility_20d' in df_result.columns:
            df_result['alpha_001'] = df_result['returns_1d'] / (df_result['volatility_20d'] + 1e-8)
            factor_count += 1
        
        if 'volume_ratio' in df_result.columns and 'rsi_14' in df_result.columns:
            df_result['alpha_002'] = df_result['volume_ratio'] * df_result['rsi_14']
            factor_count += 1
        
        if 'turnover_rate' in df_result.columns and 'returns_5d' in df_result.columns:
            df_result['alpha_003'] = df_result['turnover_rate'] * np.sign(df_result['returns_5d'])
            factor_count += 1
        
        print(f"âœ… å…±è®¡ç®—äº† {factor_count} ä¸ªå› å­")
        
        # æ›´æ–°ç‰¹å¾åˆ—åˆ—è¡¨
        self.feature_cols = [c for c in df_result.columns 
                            if c.startswith('gp_factor_') or c.startswith('alpha_')]
        
        return df_result
    
    def train_gru(self, train_df: pd.DataFrame, valid_df: pd.DataFrame):
        """
        è®­ç»ƒ GRU æ¨¡å‹
        
        Args:
            train_df: è®­ç»ƒæ•°æ®
            valid_df: éªŒè¯æ•°æ®
        """
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ GRU æ¨¡å‹...")
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = RollingDataset(train_df, self.feature_cols, 
                                       step_len=GRU_CONFIG['step_len'])
        valid_dataset = RollingDataset(valid_df, self.feature_cols,
                                       step_len=GRU_CONFIG['step_len'])
        
        train_loader = DataLoader(train_dataset, 
                                  batch_size=GRU_CONFIG['batch_size'],
                                  shuffle=True, num_workers=0)
        valid_loader = DataLoader(valid_dataset,
                                  batch_size=GRU_CONFIG['batch_size'],
                                  shuffle=False, num_workers=0)
        
        print(f"   è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        print(f"   éªŒè¯æ ·æœ¬: {len(valid_dataset)}")
        print(f"   è¾“å…¥ç‰¹å¾æ•°: {len(self.feature_cols)}")
        
        # åˆ›å»ºæ¨¡å‹
        self.gru_model = SimpleGRU(
            input_size=len(self.feature_cols),
            hidden_size=GRU_CONFIG['hidden_size'],
            num_layers=GRU_CONFIG['num_layers'],
            dropout=GRU_CONFIG['dropout']
        ).to(self.device)
        
        criterion = nn.MSELoss()
        optimizer = optim.Adam(self.gru_model.parameters(), 
                               lr=GRU_CONFIG['learning_rate'])
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=2
        )
        
        best_ic = -1
        patience_counter = 0
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(GRU_CONFIG['epochs']):
            # è®­ç»ƒ
            self.gru_model.train()
            train_loss = 0
            train_count = 0
            
            for features, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
                features = features.to(self.device)
                labels = labels.to(self.device)
                
                # æ•°æ®æ¸…æ´—
                features = torch.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
                labels = torch.nan_to_num(labels, nan=0.0, posinf=0.0, neginf=0.0)
                
                optimizer.zero_grad()
                outputs = self.gru_model(features)
                loss = criterion(outputs, labels)
                
                if torch.isnan(loss):
                    continue
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.gru_model.parameters(), max_norm=5.0)
                optimizer.step()
                
                train_loss += loss.item()
                train_count += 1
            
            avg_train_loss = train_loss / train_count if train_count > 0 else 0
            
            # éªŒè¯
            valid_ic = self.evaluate_gru(valid_loader)
            scheduler.step(valid_ic)
            
            print(f"   Epoch {epoch+1}/{GRU_CONFIG['epochs']} | "
                  f"Loss: {avg_train_loss:.6f} | Valid Rank IC: {valid_ic:.4f}")
            
            # æ—©åœ
            if valid_ic > best_ic:
                best_ic = valid_ic
                patience_counter = 0
                self.save_model("gru_best.pth")
            else:
                patience_counter += 1
                if patience_counter >= GRU_CONFIG['early_stopping_patience']:
                    print(f"   â¹ï¸ æ—©åœè§¦å‘ï¼Œæœ€ä½³ Valid IC: {best_ic:.4f}")
                    break
        
        print(f"\nâœ… GRU è®­ç»ƒå®Œæˆï¼Œæœ€ä½³ Valid Rank IC: {best_ic:.4f}")
    
    def evaluate_gru(self, data_loader: DataLoader) -> float:
        """
        è¯„ä¼° GRU æ¨¡å‹
        
        Returns:
            Rank IC
        """
        self.gru_model.eval()
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for features, labels in data_loader:
                features = features.to(self.device)
                features = torch.nan_to_num(features, nan=0.0)
                
                outputs = self.gru_model(features)
                all_preds.extend(outputs.cpu().numpy())
                all_labels.extend(labels.numpy())
        
        preds = np.array(all_preds)
        labels = np.array(all_labels)
        
        # è®¡ç®— Rank IC
        mask = ~(np.isnan(preds) | np.isnan(labels))
        if mask.sum() < 10:
            return 0.0
        
        p_rank = pd.Series(preds[mask]).rank()
        l_rank = pd.Series(labels[mask]).rank()
        
        ic = np.corrcoef(p_rank, l_rank)[0, 1]
        return ic if not np.isnan(ic) else 0.0
    
    def predict(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        é¢„æµ‹
        
        Args:
            df: è¾“å…¥æ•°æ®ï¼ˆå·²åŒ…å« GP å› å­ï¼‰
            
        Returns:
            æ·»åŠ é¢„æµ‹åˆ—çš„ DataFrame
        """
        self.gru_model.eval()
        
        dataset = RollingDataset(df, self.feature_cols, 
                                 step_len=GRU_CONFIG['step_len'])
        loader = DataLoader(dataset, batch_size=GRU_CONFIG['batch_size'],
                           shuffle=False, num_workers=0)
        
        all_preds = []
        with torch.no_grad():
            for features, _ in loader:
                features = features.to(self.device)
                features = torch.nan_to_num(features, nan=0.0)
                outputs = self.gru_model(features)
                all_preds.extend(outputs.cpu().numpy())
        
        # å°†é¢„æµ‹å€¼åˆå¹¶å› DataFrame
        # æ³¨æ„ï¼šç”±äºæ»‘åŠ¨çª—å£ï¼Œé¢„æµ‹å€¼æ¯”åŸå§‹æ•°æ®å°‘ step_len-1 æ¡
        df_result = df.copy()
        df_result['pred'] = np.nan
        
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æŒ‰æ—¥æœŸå¯¹é½
        valid_len = len(all_preds)
        df_result.iloc[-valid_len:, df_result.columns.get_loc('pred')] = all_preds
        
        return df_result
    
    def save_model(self, filename: str):
        """ä¿å­˜æ¨¡å‹"""
        if self.gru_model is not None:
            filepath = os.path.join(MODEL_OUTPUT_DIR, filename)
            torch.save(self.gru_model.state_dict(), filepath)
            print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {filepath}")
    
    def load_model(self, filename: str):
        """åŠ è½½æ¨¡å‹"""
        filepath = os.path.join(MODEL_OUTPUT_DIR, filename)
        if os.path.exists(filepath) and self.gru_model is not None:
            self.gru_model.load_state_dict(torch.load(filepath))
            print(f"ğŸ“¥ æ¨¡å‹å·²åŠ è½½: {filepath}")


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œä¸¤é˜¶æ®µæ¨¡å‹"""
    print("=" * 60)
    print("ğŸš€ ä¸¤é˜¶æ®µæ¨¡å‹ï¼šGPå› å­ + GRU")
    print("=" * 60)
    
    # 1. åŠ è½½æ•°æ®
    loader = StockDataLoader()
    df_raw = loader.load_all_data()
    df_features = loader.prepare_features(df_raw)
    df_labeled = loader.prepare_labels(df_features)
    
    # 2. åˆ’åˆ†æ•°æ®é›†
    train_df = df_labeled[df_labeled['trade_date'] <= TRAIN_END]
    valid_df = df_labeled[(df_labeled['trade_date'] > TRAIN_END) & 
                           (df_labeled['trade_date'] <= VALID_END)]
    test_df = df_labeled[df_labeled['trade_date'] > VALID_END]
    
    # 3. åˆ›å»ºä¸¤é˜¶æ®µæ¨¡å‹
    model = TwoStageModel()
    
    # 4. åŠ è½½æˆ–æŒ–æ˜ GP å› å­
    model.load_gp_factors()
    if not model.gp_programs:
        print("âš ï¸ æœªæ‰¾åˆ° GP å› å­ï¼Œä½¿ç”¨åŸºç¡€ç‰¹å¾")
        base_features = ['returns_1d', 'returns_5d', 'volatility_20d', 
                        'volume_ratio', 'rsi_14', 'macd']
    else:
        base_features = ['returns_1d', 'returns_5d', 'volatility_20d',
                        'volume_ratio', 'price_position', 'rsi_14', 'macd',
                        'turnover_rate', 'pe', 'pb']
        base_features = [c for c in base_features if c in train_df.columns]
    
    # 5. è®¡ç®— GP å› å­
    train_df = model.compute_gp_factors(train_df, base_features)
    valid_df = model.compute_gp_factors(valid_df, base_features)
    test_df = model.compute_gp_factors(test_df, base_features)
    
    # 6. è®­ç»ƒ GRU
    model.train_gru(train_df, valid_df)
    
    # 7. æµ‹è¯•é›†è¯„ä¼°
    print("\nğŸ“Š æµ‹è¯•é›†è¯„ä¼°...")
    model.load_model("gru_best.pth")
    test_result = model.predict(test_df)
    
    # è®¡ç®—æµ‹è¯•é›† IC
    test_mask = ~test_result['pred'].isna()
    if test_mask.sum() > 100:
        test_ic = np.corrcoef(
            test_result.loc[test_mask, 'pred'].rank(),
            test_result.loc[test_mask, LABEL_COL].rank()
        )[0, 1]
        print(f"   æµ‹è¯•é›† Rank IC: {test_ic:.4f}")
    
    loader.close()
    
    print("\n" + "=" * 60)
    print("âœ… ä¸¤é˜¶æ®µæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
